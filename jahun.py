import streamlit as st
import pandas as pd
import os
import json
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.schema import Document
from langchain.prompts import PromptTemplate
import re

# --- ì„¤ì • ---
GOOGLE_API_KEY = "AIzaSyA4H_SJa3vXNSdPge8PujTHuFrkOAjm2mw"
genai.configure(api_key=GOOGLE_API_KEY)

LLM_MODEL = "models/gemini-2.0-flash"
EMBEDDING_MODEL = "models/embedding-001"
JSON_PATH_MAIN = r"C:/Users/User/Desktop/budda/bdc_keywords_structured.json"
JSON_PATH_GLOSSARY = r"C:/Users/User/Desktop/budda//bdword_structured.json"


# --- ë¬¸ì„œ ë¡œë”© + TextSplitter ---
@st.cache_resource
def load_and_split_documents(main_path, glossary_path):
    raw_docs = []
    for path in [main_path, glossary_path]:
        with open(path, "r", encoding="utf-8") as f:
            json_data = json.load(f)
        for entry in json_data:
            content_parts = [
                f"chunk: {entry.get('chunk', '')}",
                f"\nì œëª©: {entry.get('title', '')}",
                f"\në‚´ìš©: {entry.get('content', '')}",
                f"\nìš”ì•½: {entry.get('summary', '')}",
                f"\ní‚¤ì›Œë“œ: {', '.join(entry.get('keyword', []))}" if isinstance(entry.get("keyword"), list) else f"\ní‚¤ì›Œë“œ: {entry.get('keyword', '')}",
                f"\nURL: {entry.get('url', '')}",
                f"\nì´ë¯¸ì§€: {entry.get('image', '')}"
            ]
            full_content = "\n".join(content_parts)

            raw_docs.append(
                Document(
                    page_content=full_content,
                    metadata={
                        "title": entry.get("title", ""),
                        "url": entry.get("url", ""),
                        "chunk": entry.get("chunk", ""),
                        "source": "json"
                    }
                )
            )

    splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    return splitter.split_documents(raw_docs)

# --- ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ---
@st.cache_resource(show_spinner=False)
def create_vector_store(_docs):
    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBEDDING_MODEL,
        google_api_key=GOOGLE_API_KEY
    )
    vector_store = FAISS.from_documents(_docs, embeddings)
    return vector_store

# --- í‚¤ì›Œë“œ ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ í•¨ìˆ˜ ---
def search_documents_by_keyword(query, documents, window=2):
    keyword_candidates = re.findall(r'\w+', query)
    found_indices = set()

    for i, doc in enumerate(documents):
        for keyword in keyword_candidates:
            if keyword in doc.page_content:
                start = max(0, i - window)
                end = min(len(documents), i + window + 1)
                found_indices.update(range(start, end))

    return [documents[i] for i in sorted(found_indices)]

# --- í”„ë¡¬í”„íŠ¸ ì •ì˜ ---
qa_template = PromptTemplate.from_template("""
ë„ˆëŠ” ë¶ˆêµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜. ê°€ëŠ¥í•œ í•œ í’ë¶€í•œ ë‚´ìš©ì„ í¬í•¨í•´ ì•Œë ¤ì¤˜.
íŠ¹íˆ ì¸ë¬¼ì´ë‚˜ ì—­ì‚¬, ë°°ê²½ì§€ì‹ ê°™ì€ ì •ë³´ë„ ì¶©ë¶„íˆ í™œìš©í•˜ì—¬ ë‹µë³€ì— í¬í•¨ì‹œì¼œ.
**í•µì‹¬ ë‹µë³€**
ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€

**ìƒì„¸ ì„¤ëª…**
ê´€ë ¨ chunk ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ìì„¸í•œ ì„¤ëª…

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸:
{question}

ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€:
""")

# --- Streamlit UI ---
st.set_page_config(page_title="ë¶ˆêµ ì±—ë´‡", layout="wide")
st.title("ë¶ˆêµí•™ìˆ ì›")

# --- ë¬¸ì„œ ë¡œë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ---
with st.spinner("ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
    documents = load_and_split_documents(JSON_PATH_MAIN, JSON_PATH_GLOSSARY)
with st.spinner("ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘..."):
    vector_store = create_vector_store(documents)

# --- LLM ì´ˆê¸°í™” ---
if "chain" not in st.session_state:
    llm = ChatGoogleGenerativeAI(
        model=LLM_MODEL,
        google_api_key=GOOGLE_API_KEY,
        temperature=0.2
    )
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 10}
    )

    chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        memory=memory,
        verbose=False
    )

    st.session_state.chain = chain
    st.session_state.messages = []
    st.session_state.retriever = retriever
    st.session_state.docs = documents
    st.session_state.llm = llm  # llm ê°ì²´ ì§ì ‘ ì €ì¥

# --- ëŒ€í™” ê¸°ë¡ ì¶œë ¥ ---
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# --- ì‚¬ìš©ì ì§ˆë¬¸ ì²˜ë¦¬ ---
if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                keyword_docs = search_documents_by_keyword(prompt, st.session_state.docs)
                source_text = ""
                unique_refs = {}

                if keyword_docs:
                    context = "\n\n".join([doc.page_content for doc in keyword_docs])
                    prompt_text = qa_template.format(context=context, question=prompt)
                    response = st.session_state.llm.invoke(prompt_text)
                    answer = response.content

                    for doc in keyword_docs:
                        title = doc.metadata.get("title", "")
                        url = doc.metadata.get("url", "")
                        if title and title not in unique_refs:
                            unique_refs[title] = url
                else:
                    result = st.session_state.chain({"question": prompt})
                    answer = result["answer"]
                    source_docs = result.get("source_documents", [])
                    for doc in source_docs:
                        title = doc.metadata.get("title", "")
                        url = doc.metadata.get("url", "")
                        if title and title not in unique_refs:
                            unique_refs[title] = url

                if unique_refs:
                    source_text = "\n\n---\n#### ğŸ“š ì°¸ê³  ë¬¸ì„œ (Top 5):\n"
                    top_refs = list(unique_refs.items())[:5]
                    for title, url in top_refs:
                        if url:
                            source_text += f"- [{title}]({url})\n"
                        else:
                            source_text += f"- {title}\n"

                st.markdown(answer + source_text)
                st.session_state.messages.append({"role": "assistant", "content": answer + source_text})

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")