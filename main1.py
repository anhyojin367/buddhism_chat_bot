import streamlit as st
import json
import re
import google.generativeai as genai
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.prompts import PromptTemplate

# --- ì„¤ì • ---
GOOGLE_API_KEY =  "AIzaSyCx-2QgdGbWxbx8RcNfz7vMPQw5itWBQuo"
genai.configure(api_key=GOOGLE_API_KEY)

LLM_MODEL = "models/gemini-2.0-flash"
EMBEDDING_MODEL = "models/embedding-001"
JSON_PATH = r"C:\Users\User\Desktop\budda\bdc_keywords_structured.json"

# --- Prompt í…œí”Œë¦¿ ê°œì„  ---
qa_template = PromptTemplate.from_template("""
ë„ˆëŠ” ë¶ˆêµ ì „ë¬¸ê°€ì•¼. ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ìì„¸í•˜ê²Œ ì„¤ëª…í•´ì¤˜. ê°€ëŠ¥í•œ í•œ í’ë¶€í•œ ë‚´ìš©ì„ í¬í•¨í•´ ì•Œë ¤ì¤˜.
íŠ¹íˆ ì¸ë¬¼ì´ë‚˜ ì—­ì‚¬, ë°°ê²½ì§€ì‹ ê°™ì€ ì •ë³´ë„ ì¶©ë¶„íˆ í™œìš©í•˜ì—¬ ë‹µë³€ì— í¬í•¨ì‹œì¼œ.
**í•µì‹¬ ë‹µë³€**
ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì´ê³  ëª…í™•í•œ ë‹µë³€

**ìƒì„¸ ì„¤ëª…**
ê´€ë ¨ chunk ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ìì„¸í•œ ì„¤ëª…

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸:
{question}

ì •í™•í•˜ê³  ê°„ê²°í•œ ë‹µë³€:
""")


# --- ë¬¸ì„œ ë¡œë”© ë° ë¶„í•  ---
@st.cache_resource
def load_and_split_documents(json_path):
    raw_docs = []
    with open(json_path, "r", encoding="utf-8") as f:
        json_data = json.load(f)

    for entry in json_data:
        content = f"ì œëª©: {entry.get('title')}\në‚´ìš©: {entry.get('content')}\nìš”ì•½: {entry.get('summary')}\ní‚¤ì›Œë“œ: {', '.join(entry.get('keyword', []))}\nURL: {entry.get('url')}"
        raw_docs.append(Document(
            page_content=content,
            metadata={
                "title": entry.get("title"),
                "url": entry.get("url"),
                "chunk": entry.get("chunk")
            }
        ))

    splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    return splitter.split_documents(raw_docs)


# --- ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ---
@st.cache_resource
def create_vector_store(_docs):
    embeddings = GoogleGenerativeAIEmbeddings(model=EMBEDDING_MODEL, google_api_key=GOOGLE_API_KEY)
    return FAISS.from_documents(_docs, embeddings), embeddings


# --- í‚¤ì›Œë“œ ì¶”ì¶œ ê°œì„  ---
def extract_keyword_from_query(query):
    cleaned = re.sub(r'(ì— ëŒ€í•´|ì´ë€|ë€|ì„|ë¥¼|ì—|ì„¤ëª…|ì•Œë ¤|í•´ì¤˜|ì¤˜|ì£¼ì„¸ìš”).*', '', query)
    return cleaned.strip()


# --- ë¬¸ì„œ ê²€ìƒ‰ ---
def find_docs_by_keyword_title(docs, keyword):
    matched_titles = {doc.metadata['title'] for doc in docs if keyword.lower() in doc.page_content.lower()}
    return [doc for doc in docs if doc.metadata['title'] in matched_titles]


# --- ì§ˆë¬¸ ì„ë² ë”© ---
def embed_query(llm_embeddings, query_text):
    return llm_embeddings.embed_query(query_text)


# --- Streamlit UI ---
st.set_page_config(page_title="ë¶ˆêµ ì±—ë´‡", layout="wide")
st.title("ë¶ˆêµ ì±—ë´‡")
st.markdown("ë¶ˆêµ ë¬¸ì„œ ê¸°ë°˜ì˜ í‚¤ì›Œë“œ ê²€ìƒ‰ + RAG")

with st.spinner("ë¬¸ì„œ ë¡œë”© ì¤‘..."):
    documents = load_and_split_documents(JSON_PATH)
vector_store, embeddings = create_vector_store(documents)

llm = ChatGoogleGenerativeAI(model=LLM_MODEL, google_api_key=GOOGLE_API_KEY, temperature=0.1)

if "messages" not in st.session_state:
    st.session_state.messages = []

# --- ëŒ€í™” UI ---
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).markdown(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ ì…ë ¥..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    st.chat_message("user").markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            try:
                keyword = extract_keyword_from_query(prompt)
                matched_docs = find_docs_by_keyword_title(documents, keyword)

                if not matched_docs:
                    matched_docs = vector_store.similarity_search(prompt, k=5)

                query_embedding = embed_query(embeddings, prompt)
                subset_store = FAISS.from_documents(matched_docs, embeddings)
                retrieved_docs = subset_store.similarity_search_by_vector(query_embedding, k=5)

                context = "\n\n".join(doc.page_content for doc in retrieved_docs)
                response = llm.invoke(qa_template.format(context=context, question=prompt))
                answer = response.content

                sources = "\n\n---\n#### ğŸ“š ì°¸ê³  ë¬¸ì„œ:\n"
                sources += "\n".join(f"- [{doc.metadata['title']}]({doc.metadata['url']})" for doc in retrieved_docs)

                st.markdown(answer + sources)
                st.session_state.messages.append({"role": "assistant", "content": answer + sources})

            except Exception as e:
                st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
